{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence and Modern Physics: a Two Way Connection\n",
    "\n",
    "The following is part of the hands-on sessions of the [AIPhy](https://aiphy.fisica.unimib.it/) school.\n",
    "The notebook aims at giving an **introduction to machine learning** methods in Python.\n",
    "Tutorials deal with different **unsupervised and supervised algorithms**.\n",
    "Students will learn how to build these algorithms from scratch using basic Python classes.\n",
    "They will then apply different techniques to test and evaluate them on toy and real world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Actions\n",
    "\n",
    "I recommend you use a Python **virtual environment** to setup your work area:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "At the time of writing the Python version is `3.10.12`: you can change this either with your distribution package manager, or by installing a **Conda** environment (e.g. `conda create -y -n aiphy python\"==3.10.12\" && conda activate aiphy`).\n",
    "\n",
    "You can then activate it with:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Before we begin, remember to install the requirements:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "We shall use mainly the `numpy` library for the algorithms, and the `matplotlib` library for plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural Networks (NNs) are a family of algorithms, usually trained using (mini-batch) gradient descent, capable of producing accurate predictions.\n",
    "Their training procedure is based on the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm, which implements the _chain rule_ of derivation to compute the gradient of a **loss function** with respect to the different parameters of the function.\n",
    "\n",
    "In simpler terms, a NN is a function\n",
    "\n",
    "$$\n",
    "f\\; \\colon\\; \\mathbb{R}^{w_{(0)}} \\to \\mathbb{R}^{w_{(N-1)}},\n",
    "$$\n",
    "\n",
    "which is the result of the composition of multiple functions $g^{(n)}\\; \\colon\\; \\mathbb{R}^{w_{(n-1)}} \\to \\mathbb{R}^{w_{(n)}}$:\n",
    "\n",
    "$$\n",
    "f = g^{(N-1)} \\circ g^{(N-2)} \\circ \\dots \\circ g^{(0)}.\n",
    "$$\n",
    "\n",
    "These functions $g^{(n)}$ are called **layers** of the NN, and are such that\n",
    "\n",
    "- the function $f$ is continuous and differentiable,\n",
    "- the function $f$ can model **non linear** functions,\n",
    "- we can apply the _chain rule_ to compute the gradient descent of the function $f$.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "The \"bricks\" $g^{(n)}$ are non linear functions, often modelled as the composition of an _affine_ application, represented by a _weight_ $W$ and a _bias_ $b$\n",
    "\n",
    "$$\n",
    "z^{(n)} = W^{(n)}\\, x^{(n-1)} + b^{(n)},\n",
    "$$\n",
    "\n",
    "and a non linear _activation function_ $a^{(n)}$ (their nature can differ according to the type of network we are dealing with):\n",
    "\n",
    "$$\n",
    "g^{(n)}(z^{(n)}) = a^{(n)}(z^{(n)}).\n",
    "$$\n",
    "\n",
    "The network is finally _trained_ using a **loss function** (or _Lagrangian_), used to minimise the \"distance\" to a target:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\left( y, f(x) \\right) = \\mathbb{E}\\left[ \\mathfrak{F}(y, f(x)) \\right],\n",
    "$$\n",
    "\n",
    "where $\\mathfrak{F}$ is a function of the **targets** (labels) $y$ and the **predictions** $f(x)$ of the network (which directly depend on the parameters).\n",
    "\n",
    "The **backpropagation** has the objective to compute the gradient of the loss function with respect to the parameters, in order to update them according to the steepest descent:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "W^{(n)} & \\gets W^{(n)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{(n)}}\n",
    "\\\\\n",
    "b^{(n)} & \\gets b^{(n)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{(n)}}\n",
    "\\end{cases}\n",
    "\\quad\n",
    "\\forall n = 1, 2, \\dots, N.\n",
    "$$\n",
    "\n",
    "This can be implemented by:\n",
    "\n",
    "1. _moving forward_ by computing the prediction $f(x)$ and the value of the loss function $\\mathcal{L}(y, f(x))$,\n",
    "2. _tracing back_ by computing the gradients of the loss function with respect to the inputs of each layer, in **reversed order**,\n",
    "3. _updating_ the parameters $W^{(n)}$ and $b^{(n)}$ according to the steepest descent.\n",
    "\n",
    "In other words, we need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(n)}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{(n)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial z^{(n)}}{\\partial W^{(n)}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial g^{(N)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial g^{(N)}}{\\partial z^{(N)}}\\,\n",
    "\\odot\n",
    "\\frac{\\partial z^{(N)}}{\\partial g^{(N-1)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial g^{(N-1)}}{\\partial z^{(N-2)}}\\,\n",
    "\\odot\n",
    "\\dots\\,\n",
    "\\cdot\n",
    "\\frac{\\partial z^{(n)}}{\\partial W^{(n)}},\n",
    "$$\n",
    "\n",
    "where the $\\cdot$ operation identifies a matrix multiplication, while $\\odot$ is the element-wise [Hadamard](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) product, which occurs when the derivative of the activation function is computed entry-wise.\n",
    "Most of the code boils down to computing:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{(n)}}\n",
    "=\n",
    "\\delta^{(n)}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial g^{(N)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial g^{(N)}}{\\partial z^{(N)}}\\,\n",
    "\\odot\n",
    "\\frac{\\partial z^{(N)}}{\\partial g^{(N-1)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial g^{(N-1)}}{\\partial z^{(N-2)}}\\,\n",
    "\\odot\n",
    "\\dots\\,\n",
    "\\frac{\\partial g^{(n)}}{\\partial z^{(n)}}\n",
    "=\n",
    "\\delta^{(n+1)}\n",
    "\\frac{\\partial z^{(n+1)}}{\\partial z^{(n)}}\n",
    "=\n",
    "\\delta^{(n+1)}\\,\n",
    "\\odot\n",
    "\\frac{\\partial z^{(n+1)}}{\\partial g^{(n)}}\\,\n",
    "\\cdot\n",
    "\\frac{\\partial g^{(n)}}{\\partial z^{(n)}}.\n",
    "$$\n",
    "\n",
    "This is a **recursive** relation we can use to implement the gradient descent algorithm on a network of functions.\n",
    "With some smart caching in our code, this is doable efficiently.\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "Usually, NNs are trained on huge amounts of data (not our case, fortunately!).\n",
    "This presents the disadvantage of not being able to load all data in memory at once.\n",
    "One possible solution is to adopt a **mini-batch** approach, that is to split the data into smaller subsets (few samples per batch) and compute the gradient descent on each batch: even though the descent is \"biased\", this helps to speed things up, and, sometimes, to make things possible.\n",
    "We can even get to the extreme case of **stochastic** gradient descent, were the update of the parameters is compute on each sample individually (that is, when the size of the mini-batch is 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Neural Networks\n",
    "\n",
    "In what follows, we implement a **fully connected** NN, and train it with mini-batch gradient descent using Python classes.\n",
    "We first import the necessary libraries, and proceed by first building abstract classes, with a shared logic, and then implementing the methods on a case-by-case basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz  # this is only for visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Iterator, Tuple, Optional\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select a style for the plots: you can freely play with this parameter, but I like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix a random number generator for reproducibility and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Abstract Implementation\n",
    "\n",
    "We first consider the structure of the network: we shall need different types of components, such as **layers**, **activations**, and **loss functions**.\n",
    "\n",
    "#### Layers\n",
    "\n",
    "We tackle the problem of the layers.\n",
    "These are structures which contain **trainable parameters** and need to be updated during the gradient descent.\n",
    "Thus, we shall need:\n",
    "\n",
    "1. a **parameter** dictionary, where we store weights and biases,\n",
    "2. some methods useful for visualisation of the structure (such as a method to pretty print the parameters, or to return the number of parameters in the layer);\n",
    "3. a **forward** pass function to compute the output of the layer,\n",
    "4. a **backward** pass function to compute the gradient of the loss function with respect to the _inputs_ of the layer (i.e. the $\\delta^{(n)}$ in the formula above),\n",
    "5. an **update** function to update the parameters of the layer by computing the gradients of the loss function with respect to the _parameters_ of the layer (using the $\\delta^{(n)}$ in the formula above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"An abstract layer class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.params: Dict[str, NDArray] = {}\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__  # it's for visualisation...\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()  # same...\n",
    "\n",
    "    def __call__(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'layer'\n",
    "\n",
    "    @property\n",
    "    def named_parameters(self) -> Dict[str, NDArray]:\n",
    "        return self.params\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return int(sum([np.prod(p.shape) for p in self.params.values()]))\n",
    "\n",
    "    def forward(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Layer must implement the forward method!')\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Layer must implement the backward method!')\n",
    "\n",
    "    def update(self, delta: NDArray, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Update parameters of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : NDArray\n",
    "            Backpropagation gradient.\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Layer must implement the update method!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then implement a simple linear layer $z^{(n)}$:\n",
    "\n",
    "- what kind of information on $z^{(n)}$ is needed to fully characterise the function?\n",
    "- how do we initialise the parameters?\n",
    "- what is the gradient of $z^{(n)}$ with respect to the input $x^{(n)}$?\n",
    "- often the backpropagation is shown for a single sample, but how do we update the parameters of the layer using an entire mini-batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"A linear (fully connected) layer.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Number of input features.\n",
    "        out_features : int\n",
    "            Number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save the hyperparameters\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Initialize the parameters\n",
    "        self.params = {}\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'Linear(in_features={self.in_features}, out_features={self.out_features})'\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'linear'\n",
    "\n",
    "    def forward(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the linear layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the linear layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the linear layer.\n",
    "        \"\"\"\n",
    "        self._input = # YOUR CODE HERE  # maybe cache the input for the update\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the linear layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the linear layer.\n",
    "        \"\"\"\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def update(self, delta: NDArray, lr: float) -> None:\n",
    "        \"\"\"\n",
    "        Update parameters of the linear layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : NDArray\n",
    "            Backpropagation gradient.\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        \"\"\"\n",
    "        # Parameters is a dictionary, so we use a dictionary of gradients\n",
    "        self._grads = {\n",
    "            # YOUR CODE HERE\n",
    "        }\n",
    "        self.params['W'] -= lr * self._grads['W']\n",
    "        self.params['b'] -= lr * self._grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL. DO NOT DELETE IT.\n",
    "\n",
    "# Create a linear layer\n",
    "layer = Linear(in_features=32, out_features=16)\n",
    "if layer.n_params != 16 * (32 + 1):\n",
    "    raise ValueError('The  nunumber of parameters do not match!')\n",
    "\n",
    "# Test the forward pass\n",
    "y_pred = gen.normal(size=(8, 32))\n",
    "y_true = layer(y_pred)\n",
    "if y_true.shape != (8, 16):\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError('The shape of the output is not correct!')\n",
    "\n",
    "# Test the backward pass\n",
    "grad_mse = layer.backward()\n",
    "if grad_mse.shape != (32, 16):\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError('The shape of the gradient is not correct!')\n",
    "if (grad_mse != layer.named_parameters['W']).all():\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError('The gradient is not correct!')\n",
    "\n",
    "# Test the update pass\n",
    "delta = gen.normal(size=(8, 16))\n",
    "layer.update(delta, lr=0.001)\n",
    "if layer._grads['W'].shape != layer.named_parameters['W'].shape:\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError('The shape of the gradient of the weight is not correct!')\n",
    "if layer._grads['b'].shape != layer.named_parameters['b'].shape:\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError('The shape of the gradient of the bias is not correct!')\n",
    "\n",
    "# All tests passed\n",
    "print('All tests passed successfully!')\n",
    "display(Image('img/allegri_calma.gif', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations\n",
    "\n",
    "We can move to the activations.\n",
    "These functions implement the non linearity needed to model the output of the NN.\n",
    "They do not contain parameters.\n",
    "However, they play a role in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"An abstract activation layer.\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __call__(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'activation'\n",
    "\n",
    "    def forward(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Activation must implement the forward method!')\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Activation must implement the backward method!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a couple of them, starting from trivial up to the most famous of all.\n",
    "We shall consider:\n",
    "\n",
    "- an **identity** activation, which we shall use only to help us think about the structure of the NN;\n",
    "- the **ReLU** activation, omni-present and universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "    \"\"\"An identity activation layer\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'Identity()'\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'identity'\n",
    "\n",
    "    def forward(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the identity activation layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the identity activation layer.\n",
    "        \"\"\"\n",
    "        self._input = # YOUR CODE HERE\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the identity activation layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the identity activation layer.\n",
    "        \"\"\"\n",
    "        return np.ones_like(self._input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    \"\"\"A ReLU activation layer.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'ReLU()'\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'relu'\n",
    "\n",
    "    def forward(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the ReLU activation layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the ReLU activation layer.\n",
    "        \"\"\"\n",
    "        self._input = # YOUR CODE HERE\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the ReLU activation layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the ReLU activation layer.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            #YOUR CODE HERE\n",
    "        ).astype(self._input.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL. DO NOT DELETE IT.\n",
    "\n",
    "# Create the activations layers\n",
    "identity = Identity()\n",
    "relu = ReLU()\n",
    "\n",
    "# Test the forward pass\n",
    "y_pred = gen.normal(size=(8, 32))\n",
    "y_identity = identity(y_pred)\n",
    "y_relu = relu(y_pred)\n",
    "if y_identity.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the output of the identity layer is not correct!')\n",
    "if (y_identity != y_pred).all():\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('The output of the identity layer is not correct!')\n",
    "if y_relu.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the output of the ReLU layer is not correct!')\n",
    "if (y_relu < 0).any():\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('The output of the ReLU layer is not correct!')\n",
    "\n",
    "# Test the backward pass\n",
    "grad_identity = identity.backward()\n",
    "grad_relu = relu.backward()\n",
    "if grad_identity.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the gradient of the identity layer is not correct!')\n",
    "if (grad_identity != np.ones_like(y_pred)).all():\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('The gradient of the identity layer is not correct!')\n",
    "if grad_relu.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the gradient of the ReLU layer is not correct!')\n",
    "if (grad_relu != (y_pred > 0)).all():\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('The gradient of the ReLU layer is not correct!')\n",
    "\n",
    "# All tests passed successfully\n",
    "print('All tests passed successfully!')\n",
    "display(Image('img/allegri_calma.gif', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions\n",
    "\n",
    "We now implement the loss function used in the NN (often called _criterion_ in many software implementations).\n",
    "In this notebook, we shall consider a regression problem, first, thus we focus on a **mean squared error** loss.\n",
    "However, we shall also try to make it a **cross entropy** loss function for a multi-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "    \"\"\"An abstract criterion (loss function) class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __call__(self, y_pred: NDArray, y_true: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : NDArray\n",
    "            Predictions.\n",
    "        y_true : NDArray\n",
    "            Targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        return self.forward(y_pred, y_true)\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'loss'\n",
    "\n",
    "    def forward(self, y_pred: NDArray, y_true: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : NDArray\n",
    "            Predictions.\n",
    "        y_true : NDArray\n",
    "            Targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Layer must implement the forward method!')\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the loss function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Layer must implement the backward method!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Criterion):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'MSELoss()'\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'mse'\n",
    "\n",
    "    def forward(self, y_pred: NDArray, y_true: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : NDArray\n",
    "            Predictions.\n",
    "        y_true : NDArray\n",
    "            Targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the mean squared error loss.\n",
    "        \"\"\"\n",
    "        self._input = # YOUR CODE HERE\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the mean squared error loss.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the mean squared error loss.\n",
    "        \"\"\"\n",
    "        return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case of the **Cross Entropy** loss might need an introduction.\n",
    "This loss is usually applied in a multi-class (let $C$ be the number of classes) context, where the **labels** must be expressed in _one-hot_ encoding.\n",
    "That is, given a batch of targets $y \\in \\mathbb{N}^B$, where $y_i$ represents the **class** of the $i$-th sample, we convert:\n",
    "\n",
    "$$\n",
    "y\n",
    "\\quad\n",
    "\\longrightarrow\n",
    "\\quad\n",
    "y \\in \\mathbb{N}^{B \\times C},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y_{ij}\n",
    "=\n",
    "\\begin{cases}\n",
    "1 \\quad &\\text{if sample $i$ belongs to class $j$}\n",
    "\\\\\n",
    "0 \\quad &\\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "This enables the possible comparison between the predictions $\\widehat{y} \\in \\mathbb{R}^{B \\times C}$, which we interpret as _probabilities_ of belonging to a class, and the _one-hot_ encoded vector.\n",
    "Our implementation of **cross entropy** should therefore include a step of encoding of the class targets.\n",
    "\n",
    "As we introduced it in class, the **probability** vector computed in the forward pass is modelled by a **softmax** activation function in the last layer:\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\mathrm{softmax}(z) = \\frac{e^z}{\\sum\\limits_{k = 0}^{C-1} e^{z_k}}.\n",
    "$$\n",
    "\n",
    "In this implementation, we integrate the **softmax** layer _directly into the definition of the cross entropy_.\n",
    "In other words, our implementation takes a vector of _logits_ as inputs.\n",
    "As we shall see, this greatly simplifies the computations (and makes the code way more numerically stable).\n",
    "\n",
    "We start by computing the gradient of the **cross-entropy**:\n",
    "\n",
    "$$\n",
    "\\mathrm{H}(y, \\widehat{y})\n",
    "=\n",
    "- \\sum\\limits_{i = 0}^{B - 1} y_i \\ln(\\widehat{y}_i).\n",
    "$$\n",
    "\n",
    "This is simply:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial \\widehat{y}_j}\n",
    "=\n",
    "- \\frac{y_j}{\\widehat{y}_j}.\n",
    "$$\n",
    "\n",
    "However, we know that\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\mathrm{softmax}(z),\n",
    "$$\n",
    "\n",
    "and we need to compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\widehat{y}_j}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "to use the _chain rule_\n",
    "\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial z_i}\n",
    "=\n",
    "\\sum\\limits_{j = 0}^{B - 1}\n",
    "\\frac{\\partial H}{\\partial \\widehat{y}_j}\n",
    "\\frac{\\partial \\widehat{y}_j}{\\partial z_i}.\n",
    "$$\n",
    "\n",
    "This derivative is quite easy to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\widehat{y}_j}{\\partial z_i}\n",
    "=\n",
    "\\frac{e^{z_j} \\delta_{ij} \\sum\\limits_{k = 0}^{C - 1} e^{z_k} - e^{z_i} e^{z_j}}{\\left( \\sum\\limits_{k = 0}^{C - 1} e^{z_k} \\right)^2}\n",
    "=\n",
    "\\widehat{y_j} \\left( \\delta_{ij} - \\widehat{y_i} \\right).\n",
    "$$\n",
    "\n",
    "Plugging the expression into the previous, the gradient of the **cross-entropy** is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial z_i}\n",
    "=\n",
    "\\widehat{y_i} - y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Criterion):\n",
    "    \"\"\"Cross entropy loss.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_classes : int\n",
    "            The number of classes (used for one-hot encoding)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_classes = # YOUR CODE HERE\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'CrossEntropyLoss()'\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        return 'cross_entropy'\n",
    "\n",
    "    def _one_hot_encoder(self, y: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        One-hot encode the classes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : NDArray\n",
    "            The vector of classes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The one-hot encoded classes.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def _softmax(self, y: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Compute the softmax activation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : NDArray\n",
    "            A vector of logits.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A vector of probabilities.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, y_pred: NDArray, y_true: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the cross entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : NDArray\n",
    "            Predictions (vector of logits).\n",
    "        y_true : NDArray\n",
    "            Targets (vector of class labels).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the cross entropy loss.\n",
    "        \"\"\"\n",
    "        eps = np.finfo(float).eps  # avoid natural logarithm of zero\n",
    "        self._input = []  # store the inputs\n",
    "\n",
    "        # Encode the true labels\n",
    "        classes = # YOUR CODE HERE\n",
    "        self._input.append(classes)\n",
    "\n",
    "        # Activate the predictions\n",
    "        activations = # YOUR CODE HERE\n",
    "        self._input.append(activations)\n",
    "\n",
    "        return float(\n",
    "            # YOUR CODE HERE\n",
    "        )\n",
    "\n",
    "    def backward(self) -> NDArray:\n",
    "        \"\"\"\n",
    "        Backward pass of the cross entropy loss\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Gradient with respect to the input of the cross entropy loss.\n",
    "        \"\"\"\n",
    "        return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL. DO NOT DELETE IT.\n",
    "\n",
    "# Create the criterion\n",
    "mse_loss = MSELoss()\n",
    "ce_loss = CrossEntropyLoss(n_classes=3)\n",
    "\n",
    "# Test the forward pass\n",
    "y_pred = gen.normal(size=(8, 3))\n",
    "y_true = gen.normal(size=(8, 3))\n",
    "y_true_classes = np.array([0, 2, 1, 0, 1, 2, 2, 0])\n",
    "\n",
    "mse_loss_value = mse_loss(y_pred, y_true)\n",
    "ce_loss_value = ce_loss(y_pred, y_true_classes)\n",
    "if not isinstance(mse_loss_value, float):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The output of the mean squared error loss should be a scalar!')\n",
    "if mse_loss_value < 0:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The output of the mean squared error loss should be a positive scalar!'\n",
    "    )\n",
    "if not np.isclose(\n",
    "        mse_loss_value, 0.5 *\n",
    "    (np.linalg.norm(y_pred - y_true, ord=2, axis=1)**2).mean()):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The output of the mean squared error loss should be the squared error!'\n",
    "    )\n",
    "if not isinstance(ce_loss_value, float):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The output of the cross entropy loss should be a scalar!')\n",
    "\n",
    "# Test the backward pass\n",
    "mse_grad = mse_loss.backward()\n",
    "ce_grad = ce_loss.backward()\n",
    "if mse_grad.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the gradient of the mean squared loss is not correct!')\n",
    "if ce_grad.shape != y_pred.shape:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The shape of the gradient of the cross entropy loss is not correct!')\n",
    "\n",
    "# All tests passed successfully\n",
    "print('All tests passed successfully!')\n",
    "display(Image('img/allegri_calma.gif', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Neural Network\n",
    "\n",
    "Let us put all the layers together in a **NN** architecture!\n",
    "The idea is to _stack_ the layers sequentially (this is not the only possibility, but let us stick to this one for simplicity).\n",
    "\n",
    "**HOMEWORK**\n",
    "\n",
    "Imagine a \"multi-task\" NN, where multiple _heads_ (NNs) are connected to a common _backbone_ (NN):\n",
    "\n",
    "- how would you track the \"flow\" of the gradient in the network?\n",
    "- what kind of changes in the code we could make to compute the correct gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"A neural network.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *layers: Layer,\n",
    "                 criterion: Optional[Criterion] = None,\n",
    "                 lr: float = 0.001) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers : Layer\n",
    "            Layers of the network (comma separated).\n",
    "        criterion : Criterion\n",
    "            Loss function. Can be None if the network is only used for inference.\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        layer_list = []\n",
    "        for layer in self.layers:\n",
    "            layer_list.append(repr(layer))\n",
    "        if self.criterion is None:\n",
    "            return ' -> '.join(layer_list)\n",
    "        return ' -> '.join(layer_list) + ' -> ' + str(self.criterion)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "    @property\n",
    "    def named_parameters(self) -> Dict[str, NDArray]:\n",
    "        \"\"\"\n",
    "        Return a dictionary of the parameters of the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, NDArray]\n",
    "            Dictionary of the parameters of the network.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            f'layer_{n:d}': layer.named_parameters\n",
    "            for (n, layer) in enumerate(self.layers)\n",
    "            if isinstance(layer, Layer)\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of parameters of the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of parameters of the network.\n",
    "        \"\"\"\n",
    "        n_params = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer):\n",
    "                n_params += layer.n_params\n",
    "        return int(n_params)\n",
    "\n",
    "    @property\n",
    "    def summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a summary of the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Summary of the network.\n",
    "        \"\"\"\n",
    "        summary = [\n",
    "            'Neural Network -- Parameter Summary',\n",
    "            '---------------------------------------',\n",
    "            '|  id  |    type    | no. parameters  |'\n",
    "        ]\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            summary.append(\n",
    "                f'| {n+1: <4d} | {layer.type: <10s} | {layer.n_params: <15d} |'\n",
    "            )\n",
    "        summary.extend([\n",
    "            '---------------------------------------',\n",
    "            f'loss function: {self.criterion.type}',\n",
    "            f'trainable parameters: {self.n_params:d}'\n",
    "        ])\n",
    "\n",
    "        return '\\n'.join(summary)\n",
    "\n",
    "    def submodel(self, stop: int = None, start: int = 0) -> 'Network':\n",
    "        \"\"\"\n",
    "        Return a submodel of the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stop : int\n",
    "            Stop index of the submodel.\n",
    "        start : int\n",
    "            Start index of the submodel.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Network\n",
    "            Submodel of the network.\n",
    "        \"\"\"\n",
    "        return Network(*self.layers[start:stop], criterion=None)\n",
    "\n",
    "    def graph(self) -> graphviz.Digraph:\n",
    "        \"\"\"\n",
    "        Return a graph representation of the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        graphviz.Digraph\n",
    "            Graph representation of the network.\n",
    "        \"\"\"\n",
    "        G = graphviz.Digraph()\n",
    "\n",
    "        # Add nodes\n",
    "        G.node('I', 'Input')\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            G.node(f'L{n}', f'{str(layer)}')\n",
    "        if self.criterion is not None:\n",
    "            G.node('output', f'{str(self.criterion)}')\n",
    "\n",
    "        # Add edges\n",
    "        G.edge('I', 'L0')\n",
    "        for n in range(len(self.layers) - 1):\n",
    "            G.edge(f'L{n}', f'L{n+1}')\n",
    "        if self.criterion is not None:\n",
    "            G.edge(f'L{n+1}', 'output')\n",
    "        return G\n",
    "\n",
    "    def __call__(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "        y : NDArray\n",
    "            Targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        return self.forward(X, y)\n",
    "\n",
    "    def predict(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Predictions of the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Predictions of the network.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, X: NDArray, y: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "        y : NDArray\n",
    "            Targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"Backward pass of the network.\"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the network we have just created.\n",
    "This is often called MultiLayer Perceptron (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Network(Linear(5, 4),\n",
    "              ReLU(),\n",
    "              Linear(4, 3),\n",
    "              ReLU(),\n",
    "              Linear(3, 2),\n",
    "              Identity(),\n",
    "              criterion=MSELoss(),\n",
    "              lr=0.0001)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have just created it, let us print a summary of the layers in the network: how many parameters can we expect to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even visualise the network graphically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "Unfortunately, we are not done, yet: the NN is nothing without data!\n",
    "Let us create a Python **iterator** capable of looping over the data.\n",
    "Specifically, let us consider a dataset of **features** $X$ and **targets** $y$, split over multiple (mini-)batches.\n",
    "\n",
    "**N.B.** a Python **iterator** is a class which implements, at least, the `__iter__` method. The `__len__` method enables the `len` function.\n",
    "\n",
    "**N.B.** the dataset we create will be split across batches in this tutorial. However, it is fully loaded into memory from the beginning: can you see advantages/disadvantages of using mini-batch gradient descent even in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Dataset of tensors (in-memory).\"\"\"\n",
    "\n",
    "    def __init__(self, X: NDArray, y: NDArray, batch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Features.\n",
    "        y : NDArray\n",
    "            Targets.\n",
    "        batch_size : int\n",
    "            Batch size.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Check if the batch size is valid\n",
    "        if batch_size > X.shape[0]:\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "        # Split the data into batches\n",
    "        # YOUR CODE HERE\n",
    "        self._features = # YOUR CODE HERE\n",
    "        self._targets = # YOUR CODE HERE\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of batches.\n",
    "        \"\"\"\n",
    "        return len(self._features)\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[NDArray, NDArray]]:\n",
    "        \"\"\"\n",
    "        Iterate over the batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Iterator[Tuple[NDArray, NDArray]]\n",
    "            Iterator over the batches.\n",
    "        \"\"\"\n",
    "        for f, t in zip(self._features, self._targets):\n",
    "            yield f, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL. DO NOT DELETE IT.\n",
    "\n",
    "# Generate some data\n",
    "y_pred = gen.normal(size=(125, 32))\n",
    "y_true = gen.normal(size=(125, 2))\n",
    "\n",
    "# Create the dataset\n",
    "data = Dataset(y_pred, y_true, batch_size=16)\n",
    "if len(data) != 125 // 16 + 1:\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('The number of batches is not correct!')\n",
    "\n",
    "data = Dataset(y_pred, y_true, batch_size=128)\n",
    "if len(data) != 1:\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'The number of batches is not correct (even though batch size is 128)!'\n",
    "    )\n",
    "\n",
    "# All tests passed successfully\n",
    "print('All tests passed successfully!')\n",
    "display(Image('img/allegri_calma.gif', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trainer\n",
    "\n",
    "Almost there!\n",
    "We need something to execute the training/validation loop of the model.\n",
    "In some software implementations, this is called a \"trainer\" and it is responsible of handling the training/validation logic, and the inference method.\n",
    "\n",
    "Ideally, we would like our trainer to contain:\n",
    "\n",
    "- the **training and validation loops** (for a single epoch, over the whole dataset split in batches),\n",
    "- a **train and validation logic**, in order to loop over the whole dataset for multiple epochs,\n",
    "- a **test and prediction logic** (i.e. something to evaluate the model over the independent test set, and something to simply output the predictions),\n",
    "- an **export** function to export the trained model to file for later use and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer for the network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Network,\n",
    "        train_data: Dataset,\n",
    "        val_data: Dataset,\n",
    "        epochs: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Network\n",
    "            Model to train.\n",
    "        train_data : Dataset\n",
    "            Training data.\n",
    "        val_data : Dataset\n",
    "            Validation data.\n",
    "        epochs : int\n",
    "            Number of epochs.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Save the losses\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def training_loop(self, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Training loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            Epoch number.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Loss.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the loss becomes NaN.\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        train = tqdm(self.train_data,\n",
    "                     total=len(self.train_data),\n",
    "                     desc=f'epoch {epoch:03d}',\n",
    "                     ncols=79,\n",
    "                     leave=True)\n",
    "        for X, y in # YOUR CODE HERE:\n",
    "\n",
    "            # Forward pass\n",
    "            loss_step = # YOUR CODE HERE\n",
    "            if np.isnan(loss_step):\n",
    "                raise ValueError(\n",
    "                    f'Some input produced NaN values during epoch {epoch:03d}!'\n",
    "                )\n",
    "            loss.append(loss_step)\n",
    "\n",
    "            # Backward pass\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = # YOUR CODE HERE\n",
    "        self.train_loss.append(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_loop(self, data: Dataset) -> float:\n",
    "        \"\"\"\n",
    "        Validation loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Dataset\n",
    "            Validation data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Loss.\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        val = tqdm(data, total=len(data), desc='validation', leave=True)\n",
    "        for X, y in # YOUR CODE HERE:\n",
    "            loss_step = # YOUR CODE HERE\n",
    "            loss.append(loss_step)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = # YOUR CODE HERE\n",
    "        self.val_loss.append(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Train and validate the model.\"\"\"\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            _ = self.training_loop(epoch)\n",
    "            _ = self.validation_loop(self.val_data)\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.val_loss = []\n",
    "        return self.validation_loop(self.val_data)\n",
    "\n",
    "    def test(self, data: Dataset) -> None:\n",
    "        \"\"\"Test the model.\"\"\"\n",
    "        self.val_loss = []\n",
    "        return self.validation_loop(data)\n",
    "\n",
    "    def predict(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Predict using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            Output of the model.\n",
    "        \"\"\"\n",
    "        return # YOUR CODE HERE\n",
    "\n",
    "    def export(self, path: str = 'model.joblib') -> None:\n",
    "        \"\"\"\n",
    "        Export the trained model.\n",
    "        \"\"\"\n",
    "        dump(self.model, path, compress=9)\n",
    "        print(f'The model has been exported to {path}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Artificial Intelligence to String Theory\n",
    "\n",
    "The example below is based **string theory**, and it has become a \"standard\" in the community.\n",
    "First, let us introduce the problem, briefly.\n",
    "\n",
    "String Theory (ST) is a candidate \"theory of everything\" capable of unifying the four fundamental interactions of nature: weak and strong nuclear forces, electromagnetism, and gravity.\n",
    "In the usual framework, fields are defined over the theoretical construct of **point particle** $x$, a 0-dimensional mathematical object which travels on a 1-dimensional trajectory, the **worldline** $x = x(\\tau)$ parametrized by the \"time\" $\\tau$.\n",
    "The main idea behind ST is the replacement of the point particle by a 1-dimensional object, called **string**, which forms a 2-dimensional trajectory, the **worldsheet** $x = x(\\tau, \\sigma)$ parametrized by time $\\tau$ and the position on the string $\\sigma$, when moving in spacetime.\n",
    "Fields are then defined using this as \"fundamental brick\".\n",
    "Observables (particles) can be defined similarly to the usual Quantum Field Theory (QFT) framework.\n",
    "However, they enjoy some of the properties offered by ST.\n",
    "\n",
    "This has some advantages (the 2D worldsheet theory is conformal, thus extremely symmetric and constrained), and some limitations.\n",
    "For instance, in QFT it is assumed that the target spacetime is $1+3$-dimensional ($D = 4$).\n",
    "In ST the dimension of spacetime is the result of a computation, and it is $D = 10$.\n",
    "This can be verified in different ways (more or less elegant):\n",
    "\n",
    "- in order to have a massless photon in the spectrum, the number of degrees of freedom (the \"little group\" of the symmetry group) must be such that $D = 10$,\n",
    "- in order to have a conformal theory without anomalies (central charge), we must require $D = 10$,\n",
    "- the request of nihilpotency of the BRST charge imposes $D = 10$,\n",
    "- etc.\n",
    "\n",
    "As a consequence, we found ourselves to be in a space with $1+9$ dimensions, only $3+1$ actually accessible at our energy.\n",
    "That is, we do not have a probe with sufficient energy to develop the entire spacetime.\n",
    "This can be modelled in the following way:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}^{1,9}\n",
    "=\n",
    "\\mathcal{M}^{1,3}\n",
    "\\times\n",
    "X_6.\n",
    "$$\n",
    "\n",
    "That is, the $1+9$-dimensional Minkowski space is, in fact, the product of the usual $1+3$-dimensional of \"everyday physics\", and a **compact** manifold $X_6$, where the remaining $6$ dimensions ($3$ in complex space $\\mathbb{C}$) are hidden.\n",
    "It goes without saying, that the \"radius\" of the compact manifold is extremely **small**.\n",
    "\n",
    "These manifolds play a central role in ST.\n",
    "They are build in order to have good **phenomenological** properties (too long to explain here), and must satisfy some requisites.\n",
    "They were first theorized by the Italian mathematician Eugenio Calabi, and later proved to exist by the Chinese mathematician Shing-Tung Yau.\n",
    "They are now known as **Calabi-Yau manifolds** (CY).\n",
    "\n",
    "\n",
    "A good part of research in ST is on such manifolds, as they are crucial for building phenomenological models.\n",
    "The different characteristics often depends on algebraic properties of the CYs.\n",
    "In particular, we are often interested by their **topology**: there exist several **topological invariants** (scalars) which completely characterise the manifold (e.g. the Euler characteristic).\n",
    "\n",
    "In what follows, we shall focus on the prediction of the **Hodge numbers** $h^{(1,1)}$ and $h^{(2,1)}$ of some particular CYs (in $3$ complex dimensions, thus CY 3-folds).\n",
    "They represent the dimensions of the cohomology groups of the manifold (in a sense, the number of holes of different dimensions in the manifold).\n",
    "Phenomenologically, they can be used to compute a number of quantities, such as the number of families of fermions in the spectrum of the theory.\n",
    "The manifolds we consider are a special class of CYs, built as **intersections** of hypersurfaces in a product of complex **projective spaces**:\n",
    "\n",
    "$$\n",
    "\\mathbb{CP}^{n_1}\n",
    "\\times\n",
    "\\mathbb{CP}^{n_2}\n",
    "\\times\n",
    "\\dots\n",
    "\\times\n",
    "\\mathbb{CP}^{n_m}.\n",
    "$$\n",
    "\n",
    "Each hypersurface is given as **homogeneous equation** with coordinates $Z^i$, $i = 0, \\dots, n_k$, for $k = 1, \\dots, m$.\n",
    "Topologically speaking, the most interesting quantity to be computed are the degrees of these equations $a^i_k$.\n",
    "By including $m$ projective spaces and $k$ equations, these can be rearranged into a matrix with integer entries:\n",
    "\n",
    "$$\n",
    "X\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a^1_1 & a^1_2 & \\dots & a^1_k \\\\\n",
    "a^2_1 & a^2_2 & \\dots & a^2_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^m_1 & a^m_2 & \\dots & a^m_k\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "These manifolds are called **Complete Intersections** CYs (CICYs).\n",
    "\n",
    "We would like to use a NN architecture to predict **at the same time**:\n",
    "\n",
    "$$\n",
    "NN\\, \\colon\\, \\mathbb{N}^{m \\times k} \\to \\mathbb{N} \\times \\mathbb{N},\n",
    "\\quad\n",
    "X \\mapsto NN(X) = (h^{(1, 1)}, h^{(2, 1)}).\n",
    "$$\n",
    "\n",
    "However, we do this by **regression**, as we are interested in finding a **function** capable of modeling the relation between the **configuration matrix** and the **Hodge numbers**, rather than a classifier, which would not generalise to unknown samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the dataset using the `pandas` library, and we split it into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/cicy3.json.gz', orient='index')\n",
    "\n",
    "# Split into training and test sets\n",
    "train = # YOUR CODE HERE\n",
    "test = # YOUR CODE HERE\n",
    "print('Train set:', train.shape)\n",
    "\n",
    "# Split into validation and test sets\n",
    "val = # YOUR CODE HERE\n",
    "test = # YOUR CODE HERE\n",
    "print('Validation set:', val.shape)\n",
    "print('Test set:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to arrange the features and the targets (labels) in the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and targets\n",
    "X_train = np.array(train['matrix'].to_list())\n",
    "X_train = X_train.reshape(train.shape[0], -1)\n",
    "y_train = train[['h11', 'h21']].values\n",
    "print(f'Shape of train data: X -> {X_train.shape}, y -> {y_train.shape}')\n",
    "\n",
    "in_features = X_train.shape[1]  # save the number of input features for later\n",
    "out_features = y_train.shape[1]  # store the shape of the labels for later\n",
    "\n",
    "X_val = np.array(val['matrix'].to_list())\n",
    "X_val = X_val.reshape(val.shape[0], -1)\n",
    "y_val = val[['h11', 'h21']].values\n",
    "print(f'Shape of val data: X -> {X_val.shape}, y -> {y_val.shape}')\n",
    "\n",
    "X_test = np.array(test['matrix'].to_list())\n",
    "X_test = X_test.reshape(test.shape[0], -1)\n",
    "y_test = test[['h11', 'h21']].values\n",
    "print(f'Shape of test data: X -> {X_test.shape}, y -> {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise the configuration matrices of a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 5), layout='constrained')\n",
    "\n",
    "ax[0].imshow(X_train[0].reshape(12, 15))\n",
    "ax[0].set_title(\n",
    "    f'$h^{{(1,1)}} = ${y_train[0,0]} | $h^{{(2,1)}} = ${y_train[0,1]}')\n",
    "ax[1].imshow(X_train[1].reshape(12, 15))\n",
    "ax[1].set_title(\n",
    "    f'$h^{{(1,1)}} = ${y_train[1,0]} | $h^{{(2,1)}} = ${y_train[1,1]}')\n",
    "ax[2].imshow(X_train[2].reshape(12, 15))\n",
    "ax[2].set_title(\n",
    "    f'$h^{{(1,1)}} = ${y_train[2,0]} | $h^{{(2,1)}} = ${y_train[2,1]}')\n",
    "\n",
    "fig.suptitle(\n",
    "    'Complete Intersection Calaby-Yau Manifolds | Configuration Matrices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.**: notice that the NN architecture is \"fully connected\" and accepts only tabular inputs.\n",
    "We shall predict the Hodge numbers using the vectorised version of the images, built by concatenating the lines of the configuration matrices one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we normalise the features, as it is always a good idea to help the machine learning algorithms by having all inputs of the same magnitude.\n",
    "\n",
    "**N.B.**\n",
    "\n",
    "All `scikit-learn` objects have roughly the same interface:\n",
    "\n",
    "- `<object>.fit(X, y)` will fit the `<object>` on features $X$ and targets $y$,\n",
    "- `<object>.transform(y)` will aplly the transformation fitted before on targets $y$ (the `<object>` must have been fitted),\n",
    "- `<object>.fit_transform(X, y)` will first call the `fit` method, then apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the inputs\n",
    "scaler = StandardScaler()  # play around with other methods!\n",
    "X_train = # YOUR CODE HERE\n",
    "X_val = # YOUR CODE HERE\n",
    "X_test = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build the datasets using the classes we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = # YOUR CODE HERE\n",
    "print(f'Lenght of train data: {len(train)} batches')\n",
    "val = # YOUR CODE HERE\n",
    "print(f'Lenght of val data: {len(val)} batches')\n",
    "test = # YOUR CODE HERE\n",
    "print(f'Lenght of test data: {len(test)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Task\n",
    "\n",
    "We build the NN using the `Network` class.\n",
    "We choose a regression loss function, as previously anticipated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [  # play around with other architectures\n",
    "    Linear(in_features, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, out_features),\n",
    "    # Identity()\n",
    "]\n",
    "\n",
    "model = Network(*layers, criterion= # YOUR CODE HERE\n",
    ", lr=0.001)\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the network! \n",
    "Let us define a suitable number of epochs, and train the network using the `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150  # play around with this!\n",
    "trainer = Trainer(model, train, val, epochs=n_epochs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, it is usually good practice to take a look at the training and validation losses.\n",
    "What can be said about the model and its training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "# Plot the loss\n",
    "ax.plot(np.arange(1, n_epochs + 1, dtype='int'),\n",
    "        trainer.train_loss,\n",
    "        'k-',\n",
    "        label='train')\n",
    "ax.plot(np.arange(1, n_epochs + 1, dtype='int'),\n",
    "        trainer.val_loss,\n",
    "        'r--',\n",
    "        label='val')\n",
    "\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Training and validation loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let us visualise and evaluate our model.\n",
    "First, consider a classical **QQ-plot** of the predictions, to test the \"alignment\" of the predictions with the respective ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "(h11_train, h21_train) = y_train.T\n",
    "(h11_val, h21_val) = y_val.T\n",
    "\n",
    "# Predictions\n",
    "(h11_train_p, h21_train_p) = trainer.predict(X_train).T\n",
    "(h11_val_p, h21_val_p) = trainer.predict(X_val).T\n",
    "\n",
    "# Quantiles\n",
    "h11_train_qq = np.quantile(h11_train, np.linspace(0, 1, num=1000))\n",
    "h11_train_p_qq = np.quantile(h11_train_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "h21_train_qq = np.quantile(h21_train, np.linspace(0, 1, num=1000))\n",
    "h21_train_p_qq = np.quantile(h21_train_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "h11_val_qq = np.quantile(h11_val, np.linspace(0, 1, num=1000))\n",
    "h11_val_p_qq = np.quantile(h11_val_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "h21_val_qq = np.quantile(h21_val, np.linspace(0, 1, num=1000))\n",
    "h21_val_p_qq = np.quantile(h21_val_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "# QQ Plots\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 5), layout='constrained')\n",
    "\n",
    "ax[0].scatter(h11_train_qq, h11_train_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[0].plot(h11_train_qq, h11_train_qq, 'r--')\n",
    "ax[0].set_title('$h^{(1,1)}$')\n",
    "ax[0].set_xlabel('ground truth (quantiles)')\n",
    "ax[0].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "ax[1].scatter(h21_train_qq, h21_train_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[1].plot(h21_train_qq, h21_train_qq, 'r--')\n",
    "ax[1].set_title('$h^{(2,1)}$')\n",
    "ax[1].set_xlabel('ground truth (quantiles)')\n",
    "ax[1].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "fig.suptitle('Training Data')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 5), layout='constrained')\n",
    "\n",
    "ax[0].scatter(h11_val_qq, h11_val_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[0].plot(h11_val_qq, h11_val_qq, 'r--')\n",
    "ax[0].set_title('$h^{(1,1)}$')\n",
    "ax[0].set_xlabel('ground truth (quantiles)')\n",
    "ax[0].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "ax[1].scatter(h21_val_qq, h21_val_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[1].plot(h21_val_qq, h21_val_qq, 'r--')\n",
    "ax[1].set_title('$h^{(2,1)}$')\n",
    "ax[1].set_xlabel('ground truth (quantiles)')\n",
    "ax[1].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "fig.suptitle('Validation Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, evaluate the model by computing its accuracy.\n",
    "Notice that, even though the network was trained for a regression task, the predicted observables are ultimately **integers**.\n",
    "We can **round** the results and compute the metrics _as if_ we were dealing with a classification task: let us use classical classification metrics to assess the goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the predictions\n",
    "h11_train_p = np.round(h11_train_p, 0).astype('int')\n",
    "h21_train_p = np.round(h21_train_p, 0).astype('int')\n",
    "\n",
    "h11_val_p = np.round(h11_val_p, 0).astype('int')\n",
    "h21_val_p = np.round(h21_val_p, 0).astype('int')\n",
    "\n",
    "# Compute the metrics\n",
    "# YOUR CODE HERE\n",
    "\n",
    "message = [\n",
    "    'TRAINING',\n",
    "    '--------',\n",
    "    f'  > accuracy : h11 = {h11_acc_train:.1%}  | h21 = {h21_acc_train:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_train:.1%} | h21 = {h21_prec_train:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_train:.1%}  | h21 = {h21_rec_train:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_train:.1%}   | h21 = {h21_f1_train:.1%}',\n",
    "    '',\n",
    "    'VALIDATION',\n",
    "    '---------',\n",
    "    f'  > accuracy : h11 = {h11_acc_val:.1%}  | h21 = {h21_acc_val:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_val:.1%} | h21 = {h21_prec_val:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_val:.1%}  | h21 = {h21_rec_val:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_val:.1%}   | h21 = {h21_f1_val:.1%}',\n",
    "]\n",
    "print('\\n'.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, results are not optimal, but we are peaking up something.\n",
    "Let us dig deeper to see if we can improve the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step, let us take a look at the **feature maps** (the intermediate layers) of the model.\n",
    "Is a pattern starting to emerge?\n",
    "\n",
    "First, extract a submodel of the network by stopping at an intermediate layer (you can play a bit with this parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last linear layer\n",
    "submodel = model.submodel(stop=5)\n",
    "submodel.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, visualise the vectors output by the intermediate layer using **unsupervised** methods for visualisation:\n",
    "\n",
    "- as the feature map is in high dimensional space, do you know of any unsupervised algorithm for dimensionality reduction?\n",
    "- as some structure might emerge, do you know any good algorithm which could help us see whether data are grouped together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = submodel.predict(X_train)\n",
    "\n",
    "# Plot the feature maps\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans_labels = kmeans.fit_predict(feature_maps)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "feature_maps_pca = pca.fit_transform(feature_maps)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "ax.scatter(  # YOUR CODE HERE,\n",
    "    # YOUR CODE HERE,\n",
    "    alpha=0.25,\n",
    "    c=kmeans_labels,\n",
    "    cmap='tab10',\n",
    "    marker='o')\n",
    "\n",
    "ax.set_xlabel(\n",
    "    f'1st principal component ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(\n",
    "    f'2nd principal component ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('Feature Maps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, let us simulate a \"deployment\" into \"production\" of our model, and test it on unknown samples in the test set:\n",
    "\n",
    "1. save the model architecture and parameters to file, to be exported to a different machine (not in this case),\n",
    "2. load the model from file, and make predictions on the test set,\n",
    "3. as in this case ground truths are available, compute the metrics and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model, 'model.joblib')\n",
    "\n",
    "# Reload the model\n",
    "model = load('model.joblib')\n",
    "h11_test_p, h21_test_p = model.predict(X_test).T\n",
    "h11_test, h21_test = y_test.T\n",
    "\n",
    "# Round the predictions\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compute the metrics\n",
    "# YOUR CODE HERE\n",
    "\n",
    "message = [\n",
    "    'TEST',\n",
    "    '----',\n",
    "    f'  > accuracy : h11 = {h11_acc_test:.1%}  | h21 = {h21_acc_test:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_test:.1%} | h21 = {h21_prec_test:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_test:.1%}  | h21 = {h21_rec_test:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_test:.1%}   | h21 = {h21_f1_test:.1%}',\n",
    "]\n",
    "print('\\n'.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task\n",
    "\n",
    "Let us change the point of view and try a _classification_ of $h^{(1, 1)}$ based on the configuration matrix.\n",
    "This is technically a simpler task, which depends on a strong _prior knowledge_ of the range of variation of the Hodge numbers (no extrapolation possible during inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_class = y_train[..., 0]  # take only h11\n",
    "y_val_class = # YOUR CODE HERE\n",
    "y_test_class = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many classes are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then build the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = # YOUR CODE HERE\n",
    "print(f'Lenght of train data: {len(train)} batches')\n",
    "val = # YOUR CODE HERE\n",
    "print(f'Lenght of val data: {len(val)} batches')\n",
    "test = # YOUR CODE HERE\n",
    "print(f'Lenght of test data: {len(test)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct the architecture.\n",
    "How many neurons should we insert in the last linear layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(in_features, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 32),\n",
    "    ReLU(),\n",
    "    Linear(32, n_classes),\n",
    "]\n",
    "\n",
    "model = Network(*layers,\n",
    "                criterion=# YOUR CODE HERE,\n",
    "                lr=0.001)\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the network! \n",
    "Let us define a suitable number of epochs, and train the network using the `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 600\n",
    "trainer = Trainer(model, train, val, epochs=n_epochs)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous case, let us take a look at the evolution of the loss function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "# Plot the loss\n",
    "ax.plot(np.arange(1, n_epochs + 1, dtype='int'),\n",
    "        trainer.train_loss,\n",
    "        'k-',\n",
    "        label='train')\n",
    "ax.plot(np.arange(1, n_epochs + 1, dtype='int'),\n",
    "        trainer.val_loss,\n",
    "        'r--',\n",
    "        label='val')\n",
    "\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Training and validation loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then visualise the predicted and real distributions using the **QQ-plots**.\n",
    "Rember that predictions are **vectors** of probabilities: how can we recover the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "h11_train = y_train_class.ravel()\n",
    "h11_val = y_val_class.ravel()\n",
    "\n",
    "# Predictions\n",
    "h11_train_p = np.argmax(trainer.predict(X_train), axis=1)\n",
    "h11_val_p = np.argmax(trainer.predict(X_val), axis=1)\n",
    "\n",
    "# Quantiles\n",
    "h11_train_qq = np.quantile(h11_train, np.linspace(0, 1, num=1000))\n",
    "h11_train_p_qq = np.quantile(h11_train_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "h11_val_qq = np.quantile(h11_val, np.linspace(0, 1, num=1000))\n",
    "h11_val_p_qq = np.quantile(h11_val_p, np.linspace(0, 1, num=1000))\n",
    "\n",
    "# QQ Plots\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 5), layout='constrained')\n",
    "\n",
    "ax[0].scatter(h11_train_qq, h11_train_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[0].plot(h11_train_qq, h11_train_qq, 'r--')\n",
    "ax[0].set_title('$h^{(1,1)}$ (training data)')\n",
    "ax[0].set_xlabel('ground truth (quantiles)')\n",
    "ax[0].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "ax[1].scatter(h11_val_qq, h11_val_p_qq, alpha=0.25, color='k', marker='o')\n",
    "ax[1].plot(h11_val_qq, h11_val_qq, 'r--')\n",
    "ax[1].set_title('$h^{(1,1)}$ (validation data)')\n",
    "ax[1].set_xlabel('ground truth (quantiles)')\n",
    "ax[1].set_ylabel('prediction (quantiles)')\n",
    "\n",
    "fig.suptitle('QQ plots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then proceed to compute the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "# YOUR CODE HERE\n",
    "\n",
    "message = [\n",
    "    'TRAINING',\n",
    "    '--------',\n",
    "    f'  > accuracy : h11 = {h11_acc_train:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_train:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_train:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_train:.1%}',\n",
    "    '',\n",
    "    'VALIDATION',\n",
    "    '---------',\n",
    "    f'  > accuracy : h11 = {h11_acc_val:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_val:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_val:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_val:.1%}',\n",
    "]\n",
    "print('\\n'.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then take a look at the feature maps, as done before for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last linear layer\n",
    "submodel = model.submodel(stop=7)\n",
    "submodel.graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, visualise the vectors output by the intermediate layer using **unsupervised** methods for visualisation:\n",
    "\n",
    "- as the feature map is in high dimensional space, do you know of any unsupervised algorithm for dimensionality reduction?\n",
    "- as some structure might emerge, do you know any good algorithm which could help us see whether data are grouped together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = submodel.predict(X_train)\n",
    "\n",
    "# Plot the feature maps\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans_labels = kmeans.fit_predict(feature_maps)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "feature_maps_pca = pca.fit_transform(feature_maps)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "ax.scatter(feature_maps_pca[..., 0],\n",
    "           feature_maps_pca[..., 1],\n",
    "           alpha=0.25,\n",
    "           c=kmeans_labels,\n",
    "           cmap='tab20',\n",
    "           marker='o')\n",
    "\n",
    "ax.set_xlabel(\n",
    "    f'1st principal component ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(\n",
    "    f'2nd principal component ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('Feature Maps')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, let us simulate a \"deployment\" into \"production\" of our model, and test it on unknown samples in the test set:\n",
    "\n",
    "1. save the model architecture and parameters to file, to be exported to a different machine (not in this case),\n",
    "2. load the model from file, and make predictions on the test set,\n",
    "3. as in this case ground truths are available, compute the metrics and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dump(model, 'model.joblib')\n",
    "\n",
    "# Reload the model\n",
    "model = load('model.joblib')\n",
    "h11_test_p = # YOUR CODE HERE\n",
    "h11_test = y_test_class\n",
    "\n",
    "# Compute the metrics\n",
    "# YOUR CODE HERE\n",
    "\n",
    "message = [\n",
    "    'TEST',\n",
    "    '----',\n",
    "    f'  > accuracy : h11 = {h11_acc_test:.1%}',\n",
    "    f'  > precision: h11 = {h11_prec_test:.1%}',\n",
    "    f'  > recall   : h11 = {h11_rec_test:.1%}',\n",
    "    f'  > f1 score : h11 = {h11_f1_test:.1%}',\n",
    "]\n",
    "print('\\n'.join(message))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiphy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
