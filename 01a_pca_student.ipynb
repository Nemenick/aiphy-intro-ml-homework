{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c55f6e5",
   "metadata": {},
   "source": [
    "# Artificial Intelligence and Modern Physics: a Two Way Connection\n",
    "\n",
    "The following is part of the hands-on sessions of the [AIPhy](https://aiphy.fisica.unimib.it/) school.\n",
    "The notebook aims at giving an **introduction to machine learning** methods in Python.\n",
    "Tutorials deal with different **unsupervised and supervised algorithms**.\n",
    "Students will learn how to build these algorithms from scratch using basic Python classes.\n",
    "They will then apply different techniques to test and evaluate them on toy and real world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6fefd",
   "metadata": {},
   "source": [
    "## Preliminary Actions\n",
    "\n",
    "I recommend you use a Python **virtual environment** to setup your work area:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "At the time of writing the Python version is `3.10.12`: you can change this either with your distribution package manager, or by installing a **Conda** environment (e.g. `conda create -y -n aiphy python\"==3.10.12\" && conda activate aiphy`).\n",
    "\n",
    "You can then activate it with:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Before we begin, remember to install the requirements:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "We shall use mainly the `numpy` library for the algorithms, and the `matplotlib` library for plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d842cfa",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "\n",
    "**Principal Components Analysis** (PCA) is a technique introduced by Karl Pearson in [1901](https://doi.org/10.1080/14786440109462720).\n",
    "The basic idea of the technique is to find the main directions of variation in a dataset.\n",
    "This can be achieved by fitting an ellipsoid to the data which is then projected on these directions.\n",
    "\n",
    "### PCA Through Singular Value Decomposition\n",
    "\n",
    "PCA acts as a (pseudo-)rotation on the a data matrix $X = \\left( x_{(i)}^T \\right)_{i = 1, \\dots, n}$, where $x_{(i)} \\in \\mathbb{R}^p$ are column vectors:\n",
    "\n",
    "$$\n",
    "X^{\\prime} = X W.\n",
    "$$\n",
    "\n",
    "The matrix $W$ is a $p \\times p$ matrix of _loadings_ whose columns are eigenvectors of the covariance matrix $C = (n - 1)^{-1} X^T X$, which automatically ensures that:\n",
    "\n",
    "$$\n",
    "\\forall k = 1, 2, \\dots, p,\n",
    "\\quad\n",
    "\\mathrm{Var}(X \\cdot w_{[k]}) = \\lambda_{[k]}\n",
    "$$\n",
    "\n",
    "is the $k$-th largest eigenvalue of the covariance matrix.\n",
    "Hence, we can decompose:\n",
    "\n",
    "$$\n",
    "C = \\sum\\limits_{k = 1}^p \\lambda_{[k]} w_{[k]} w_{[k]}^T = W \\Lambda W^T,\n",
    "$$\n",
    "\n",
    "where $\\Lambda \\in \\mathbb{R}^{p \\times p}$ is a diagonal matrix with real nonnegative eigenvalues.\n",
    "\n",
    "On the  other hand, a different kind of _decomposition_ of matrix $X$ can be used to compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "This is done by using the [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition):\n",
    "\n",
    "$$\n",
    "X = U\\, S\\, V^T,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $U \\in \\mathbb{R}^{n \\times n}$,\n",
    "- $V \\in \\mathbb{R}^{p \\times p}$,\n",
    "- $S\\in \\mathbb{R}^{n \\times p}$ is a _rectangular diagonal matrix_\n",
    "\n",
    "$$\n",
    "S =\n",
    "\\begin{pmatrix}\n",
    "s_1 & 0   & 0 & \\dots & 0   & \\dots & 0 \\\\\n",
    "0   & s_2 & 0 & \\dots & 0   & \\dots & 0 \\\\\n",
    "\\vdots \\\\\n",
    "0   & 0   & 0 & \\dots & s_p & \\dots & 0\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Computing the **covariance** of $X$, we obtain:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X) = (n - 1)^{-1}\\, V S^2 V^T,\n",
    "$$\n",
    "\n",
    "which shows that we can identify the matrix $W$ with the matrix $V$.\n",
    "Moreover, we can identify:\n",
    "\n",
    "$$\n",
    "\\Lambda = \\frac{S^2}{n - 1}.\n",
    "$$\n",
    "\n",
    "From the PCA equation, it follows that:\n",
    "\n",
    "$$\n",
    "X = U S W^T = X^{\\prime} W^T,\n",
    "$$\n",
    "\n",
    "which tells us that the **principal components** are $X^{\\prime} = U\\, S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbaa1b",
   "metadata": {},
   "source": [
    "### Coding PCA\n",
    "\n",
    "In the following, we build the code for a complete PCA, using Python classes.\n",
    "We first import the necessary libraries, and build an abstract class of transformations and projections, capable to fit and transform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42950dd0-4383-445b-8444-1a7276ed0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a90dea-7540-4718-9e98-e7360b962485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jdc  # this is a Jupyter extension to write classes in multiple cells\n",
    "import matplotlib as mpl\n",
    "from typing import Tuple, Optional\n",
    "from numpy.typing import NDArray\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces  # only use of Scikit!\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6794b",
   "metadata": {},
   "source": [
    "We then select a style for the plots: you can freely play with this parameter, but I like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a06f695-d611-4105-bdde-a434d470a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b03cca",
   "metadata": {},
   "source": [
    "#### The Abstract Implementation\n",
    "\n",
    "We first consider the structure of the desired data transformation.\n",
    "Ideally, it needs:\n",
    "\n",
    "1. to be a Python class, capable of taking arguments as **hyperparamters**,\n",
    "2. to have a **fitting** functionality to compute the transformation based on the data received as inputs,\n",
    "3. to have a **transformation** method, capable of taking inputs and perform the same transformation.\n",
    "\n",
    "Here is an abstract implementation to be inherited by the the actual implementation of the PCA class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd2daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation:\n",
    "    \"\"\"Abstract transformation class\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._fitted = False  # store an attribute to check if fitted to data\n",
    "\n",
    "    def fit(self, X: NDArray) -> 'Transformation':\n",
    "        \"\"\"\n",
    "        Fit the transformation to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            The data to fit to the transformation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Transformation must implement the fit method!')\n",
    "\n",
    "    def transform(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Transform the data using the computed parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            The data to transform.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            'All subclasses of Transformation must implement the transform method!'\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, X: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Fit and transform the data.\n",
    "\n",
    "        This is a wrapper for the `fit` and `transform` methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            The data to fit and transform.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb1fa6",
   "metadata": {},
   "source": [
    "#### The Actual Implementation\n",
    "\n",
    "We can now proceed to implement the PCA class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e54774",
   "metadata": {},
   "source": [
    "We first start with the class constructor.\n",
    "Remember that PCA **can** be used for dimensionality reduction $\\Rightarrow$ we should consider the possibility to select only a certain number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "688e493f-229a-4a92-beaa-fe93ee5810bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(Transformation):\n",
    "    \"\"\"Principal Component Analysis (PCA)\"\"\"\n",
    "\n",
    "    def __init__(self, n_components: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components : int, optional\n",
    "            The number of components to keep. If None, all components are kept.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the number of components is less than 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity check: is the number of components valid?\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Store the number of components\n",
    "        self.n_components = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6b0a9",
   "metadata": {},
   "source": [
    "We can then move to the fitting function.\n",
    "We would like to use the **SVD** technique to compute loadings and explained variance (i.e. the eigenvalues).\n",
    "Remember that each component concurs to form the full variance of the dataset: we can express its value as a fraction of the total, too.\n",
    "\n",
    "**HINT**: check `numpy`'s submodule `linalg` to find something useful!\n",
    "\n",
    "As we would like to apply separately the PCA to different input data (e.g. to project on the same subspace), is this the good place to compute and store the principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33bb42c3-b191-49b6-a171-954b0095969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PCA\n",
    "def fit(self, X: NDArray) -> 'PCA':\n",
    "    \"\"\"\n",
    "    Fit the transformation to the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        The data to fit to the transformation.\n",
    "    \"\"\"\n",
    "    # Check that the shape of the data is valid\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError('The input data must be two-dimensional! However we received %r dimensional data.' % X.ndim)\n",
    "\n",
    "    # Check that the value of the components to keep is valid\n",
    "    if self.n_components is not None:\n",
    "        if self.n_components > X.shape[1]:\n",
    "            self.n_components = # YOUR CODE HERE\n",
    "\n",
    "    # Compute the SVD (with centred data)\n",
    "    X = X.copy()  # make sure we keep an original\n",
    "    self._mean = # YOUR CODE HERE\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Store the components\n",
    "    self.loadings_ = #YOUR CODE HERE\n",
    "\n",
    "    # Compute the explained variance and express it as ratio of the total\n",
    "    self.explained_variance_ = # YOUR CODE HERE\n",
    "    self.explained_variance_ = self.explained_variance_[:self.n_components]\n",
    "\n",
    "    norm = # YOUR CODE HERE\n",
    "    self.explained_variance_ratio_ = self.explained_variance_ / norm\n",
    "\n",
    "    # Update the fitted state\n",
    "    self._fitted = # YOUR CODE HERE\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e25a6",
   "metadata": {},
   "source": [
    "We can then move to the transformation function.\n",
    "We shall use the loadings computed in the the previous function to apply the rotation to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcadedbc-56fe-4a09-9e56-6a355c1b992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to PCA\n",
    "def transform(self, X: NDArray) -> Tuple[NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Transform the data using the computed parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        The data to transform.\n",
    "    \"\"\"\n",
    "    # Check that the transformation has been fitted to data\n",
    "    if not self._fitted:\n",
    "        raise RuntimeError('PCA must be fitted before calling the transform method!')\n",
    "\n",
    "    # Apply the transformation\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return scores, self.loadings_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cde8dc",
   "metadata": {},
   "source": [
    "Before moving on, let us test the implementation to check that everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL. DO NOT DELETE IT.\n",
    "\n",
    "# Generate some test data\n",
    "gen = np.random.default_rng(1)\n",
    "X = gen.normal(size=(3, 2))\n",
    "X -= X.mean(axis=0)  # always centre the data\n",
    "\n",
    "# Compute the covariance matrix and the eigenvalues/eigenvectors\n",
    "C = np.cov(X, rowvar=False)\n",
    "evl, evc = np.linalg.eigh(C)\n",
    "evl = evl[::-1]  # sort in descending order\n",
    "evc = evc[:, ::-1]  # sort in descending order\n",
    "\n",
    "# Compute PCA and compare the results\n",
    "pca = PCA().fit(X)\n",
    "if (not hasattr(pca, 'loadings_')) and (not hasattr(pca, 'loadings')):\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise RuntimeError(\n",
    "        'The PCA class might not have a saving method for the loadings!')\n",
    "if not pca._fitted:\n",
    "    display(Image('img/allegri_giacca.gif', width=500))\n",
    "    raise RuntimeError(\n",
    "        'The PCA class does not update the fitted state correctly!')\n",
    "if not np.allclose(pca.explained_variance_, evl):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('Difference in eigenvalues!')\n",
    "if not np.allclose(sum(pca.explained_variance_ratio_), 1.0):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError(\n",
    "        'Difference in explained variance ratios! The sum is %r instead of 1.0!'\n",
    "        % sum(pca.explained_variance_ratio_))\n",
    "if not np.allclose(evc @ pca.loadings_.T, np.identity(2)):\n",
    "    display(Image('img/allegri_dipoco.gif', width=500))\n",
    "    raise ValueError('Difference in eigenvectors!')\n",
    "\n",
    "# If everything is OK, print a message\n",
    "print('All tests passed!')\n",
    "display(Image('img/allegri_calma.gif', width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502d5f2",
   "metadata": {},
   "source": [
    "### Some Simple Test\n",
    "\n",
    "We then proceed to apply PCA on a simple test dataset to better understand its function.\n",
    "We first generate some data according to a known multivariate 2D distribution\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x} ~\\mid~ \\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
    "=\n",
    "\\frac{1}{2 \\pi \\sqrt{\\det{\\mathbf{\\Sigma}}}}\n",
    "\\exp\\left({-\\left(\\mathbf{x} - \\mathbf{\\mu}\\right)^T \\mathbf{\\Sigma}^{-1} \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}$ is the population covariance matrix, and $\\mathbf{\\mu}$ is the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6cb542-1813-4c75-aa8e-f86861c9a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.default_rng(123)\n",
    "X = gen.multivariate_normal(mean=[0, 0], cov=[[3, 3], [3, 4]], size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81cef3",
   "metadata": {},
   "source": [
    "We can then instantiate the PCA object we just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c081089f-1932-4e53-9f86-ca6302e2d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdf017",
   "metadata": {},
   "source": [
    "But how can we visualise the data? And how about the principal directions (the loadings)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efc633",
   "metadata": {},
   "source": [
    "For data, this is rather easy, as we can straightforwardly plot the data matrix $X$ using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4195d4-3a53-4f19-9c74-a5b38c9b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "ax.scatter(\n",
    "    # YOUR CODE HERE\n",
    ")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01798225",
   "metadata": {},
   "source": [
    "We the fit and transform the data, in order to obtain the principal components (scores) and the loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28f81d40-105d-4d45-81cc-8be25f852d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp, W = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff78692",
   "metadata": {},
   "source": [
    "**QUESTION**: we know that a positive semi-definite matrix $C$ can be diagonalised as $C = W \\Lambda W^T$, where $\\Lambda$ is a diagonal matrix.\n",
    "Is this transformation unique? Do you think you can identify the symmetry group?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7179a",
   "metadata": {},
   "source": [
    "We then plot the loadings on the data.\n",
    "To visualise them better, it might be appropriate to rescale them by the corresponding variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5aac8-72e6-436f-9d51-3f6db9afe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "ax.scatter(\n",
    "    # YOUR CODE HERE\n",
    "    )  # plot the data\n",
    "\n",
    "# Rescale the loadings W' = S . W\n",
    "Wp = # YOUR CODE HERE\n",
    "\n",
    "# Plot the loadings\n",
    "ax.plot(\n",
    "    # YOUR CODE HERE,\n",
    "     'r-', lw=3)\n",
    "ax.plot(\n",
    "    # YOUR CODE HERE,\n",
    "     'g-', lw=3)\n",
    "ax.plot([0], [0], 'wo', lw=3)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Loadings')\n",
    "ax.set_aspect('equal')  # this ensures that orthigonality is visible\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d78f9",
   "metadata": {},
   "source": [
    "Finally, we are ready to visualise the transformation of the data to principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f124283-f68f-4b66-99bf-4b93e435191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "ax.scatter(\n",
    "    # YOUR CODE HERE,\n",
    "    label='data',\n",
    "    alpha=0.25)\n",
    "ax.scatter(  # YOUR CODE HERE,\n",
    "    label='scores', color='r', marker='x', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\n",
    "    f'x / 1st principal component ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(\n",
    "    f'y / 2nd principal component ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_title('Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af418bb0",
   "metadata": {},
   "source": [
    "## Principal Components Analysis and Computer Vision\n",
    "\n",
    "We tackle now one of the first uses of PCA for computer vision.\n",
    "We are talking about the decomposition of human faces for people identification (see [Sirovich and Kirby (1987)](https://doi.org/10.1364%2FJOSAA.4.000519)).\n",
    "\n",
    "We shall download the [\"Olivetti AT&T\" dataset](https://cs.nyu.edu/~roweis/data.html) and analyse its decomposition with PCA.\n",
    "The dataset contains 400 faces of 40 individuals in 64x64 pixel grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0653cc-3e88-4bb1-822d-d7990fc8a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, face_ids = fetch_olivetti_faces(return_X_y=True,\n",
    "                                       shuffle=True,\n",
    "                                       random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\n",
    "    f'Test set: {X_test.shape} (i.e. {X_test.shape[0] / faces.shape[0]:.1%})')\n",
    "\n",
    "# Spare a few samples for validation\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\n",
    "    f'Validation set: {X_val.shape} (i.e. {X_val.shape[0] / X_train.shape[0]:.1%})'\n",
    ")\n",
    "print(\n",
    "    f'Training set: {X_train.shape} (i.e. {X_train.shape[0] / faces.shape[0]:.1%})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa41699",
   "metadata": {},
   "source": [
    "First, we visualise data from the training set, in order to get an idea of what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150f45b-98b1-4b94-8663-51692cd2e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(9, 9), layout='constrained')\n",
    "fig.suptitle('Olivetti AT&T Dataset')\n",
    "ax = ax.ravel()\n",
    "for n in range(9):\n",
    "    ax[n].imshow(X_train[n].reshape(64, 64))  # reshape the data to 64x64\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5faac",
   "metadata": {},
   "source": [
    "We can then apply the PCA to the data (let us keep all components).\n",
    "We shall, however, need to first centre the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70907047-622a-4cbc-968e-5dedd73b721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd5918",
   "metadata": {},
   "source": [
    "For the sake of completeness, let us visualise the disposition of the scores in different ways.\n",
    "Consider first a 3D plot of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd631b66-6e20-4b1c-8a94-c512b9c9fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5),\n",
    "                       layout='constrained',\n",
    "                       subplot_kw={'projection': '3d'})\n",
    "ax.scatter(  # YOUR CODE HERE,\n",
    "    c=y_train, cmap='Set1', marker='x')\n",
    "ax.set_xlabel(f'1st PC ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'2nd PC ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_zlabel(f'3rd PC ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "ax.set_title('Olivetti AT&T Dataset | 3D PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd29f4",
   "metadata": {},
   "source": [
    "We might have picked up something, but probably not enough in 3D.\n",
    "Let us take a look with an orthogonal projection in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d04d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(18, 5), layout='constrained')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ax[0].set_xlabel(\n",
    "    f'1st principal components ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax[0].set_ylabel(\n",
    "    f'2nd principal components ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax[1].set_xlabel(\n",
    "    f'1st principal components ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax[1].set_ylabel(\n",
    "    f'3rd principal components ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "ax[2].set_xlabel(\n",
    "    f'2nd principal components ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "ax[2].set_ylabel(\n",
    "    f'3rd principal components ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "fig.suptitle('Olivetti AT&T Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ae7bf",
   "metadata": {},
   "source": [
    "It still does not seem to be enough, even though some of the principal components seem to be grouped/clustered together.\n",
    "Let us take a closer look to the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc849fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "# Compute the cumulative sum of the explained variance ratio (call it y)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ax.plot(y, 'k-', label='cumulative')\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('explained variance')\n",
    "ax.set_title('Cumulative Explained Variance')\n",
    "ax.axvline(0, 0, 1, ls='--', alpha=0.35)\n",
    "ax.axhline(0, 0, 1, ls='--', alpha=0.35)\n",
    "ax.axvline((y >= 0.95).argmax(),\n",
    "           0,\n",
    "           1,\n",
    "           color='b',\n",
    "           ls='--',\n",
    "           alpha=0.35,\n",
    "           label=f'95% threshold ({(y >= 0.95).argmax():d} components)')\n",
    "ax.axhline(y[0],\n",
    "           0,\n",
    "           1,\n",
    "           color='r',\n",
    "           ls='--',\n",
    "           alpha=0.35,\n",
    "           label=f'1st PC ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "ax.axhline(1,\n",
    "           0,\n",
    "           1,\n",
    "           color='g',\n",
    "           ls='-.',\n",
    "           alpha=0.35,\n",
    "           label='full reconstruction')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50453727",
   "metadata": {},
   "source": [
    "Apparently, we might need well more than 3 principal components, after all.\n",
    "For completeness, let us visualise the loadings.\n",
    "\n",
    "**QUESTION**: the loadings are the coefficients of the PCA transformation.\n",
    "Their support is the original features (pixels in this case).\n",
    "How can we best visualise the loadings, in this scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aee5d7-8b0c-473c-a94e-5f666a706abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3,\n",
    "                       nrows=3,\n",
    "                       figsize=(12, 15),\n",
    "                       layout='constrained')\n",
    "fig.suptitle('Olivetti AT&T Dataset')\n",
    "ax = ax.ravel()\n",
    "for n in range(9):\n",
    "    ax[n].imshow(loadings[..., n].reshape(64, 64))\n",
    "    ax[n].set_title(\n",
    "        f'Loading vector {n+1:d} ({pca.explained_variance_ratio_[n]:.1%})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2575b",
   "metadata": {},
   "source": [
    "### An Attempt at Classification\n",
    "\n",
    "Using the principal components, we try to classify images using a [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5ab6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18807842",
   "metadata": {},
   "source": [
    "In particular, we use look for the best number of components to use, through validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e56413",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'n_components': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "clf = LogisticRegression()\n",
    "for n in range(10, 50):  # number of components\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Make predictions\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute metrics\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Convert to array\n",
    "metrics = {k: np.array(v) for k, v in metrics.items()}\n",
    "x_best, y_best = np.argmax(metrics['f1']) + 10, np.max(metrics['f1'])\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(6, 5), layout='constrained')\n",
    "\n",
    "## Metrics\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "## Best model line\n",
    "ax.axvline(x_best,\n",
    "           0,\n",
    "           1,\n",
    "           color='k',\n",
    "           ls='--',\n",
    "           label=f'best model ({x_best:d} comp. | F1 = {y_best:.1%})')\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('score')\n",
    "ax.set_title('Validation Metrics')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369a1dd",
   "metadata": {},
   "source": [
    "Finally, we predict the test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test,\n",
    "                                                           y_pred,\n",
    "                                                           average='macro')\n",
    "print(f'Precision: {precision:.2%}')\n",
    "print(f'Recall: {recall:.2%}')\n",
    "print(f'F1: {f1:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
